{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1CM3wA8o_sOVaoeugSkwHVaWbvkWxW_UI","timestamp":1745951262014}],"gpuType":"T4","machine_shape":"hm","mount_file_id":"1CM3wA8o_sOVaoeugSkwHVaWbvkWxW_UI","authorship_tag":"ABX9TyNxap9CUb4wNR+L0KqqAIjE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Importações"],"metadata":{"id":"ThjIwOL60Ovf"}},{"cell_type":"code","source":["# Importação das bibliotecas necessárias\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras.models import Sequential, save_model, load_model\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","import tensorflow as tf\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n"],"metadata":{"id":"JJeTEwDVhwm-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preparação e normalização dos dados e geração dos rótulos"],"metadata":{"id":"dkaSXn1g0sEL"}},{"cell_type":"code","source":["# Pastas do dataset\n","dataset_dir = '/content/drive/MyDrive/Entregavel_2/imagens'\n","drone_dir = os.path.join(dataset_dir, '/content/drive/MyDrive/Entregavel_2/imagens/drone')\n","trator_dir = os.path.join(dataset_dir, '/content/drive/MyDrive/Entregavel_2/imagens/trator')\n","\n","# Função para carregar imagens e gerar rótulols\n","def carregar_dataset(drone_dir, trator_dir):\n","    imagens = []\n","    rotulos = []\n","\n","    # Carregar imagens de drones (classe 0)\n","    for img_name in os.listdir(drone_dir):\n","        if img_name.endswith(('.jpg', '.jpeg', '.png')):\n","            img_path = os.path.join(drone_dir, img_name)\n","            img = load_img(img_path, target_size=(150, 150))\n","            img_array = img_to_array(img) / 255.0\n","            imagens.append(img_array)\n","            rotulos.append(0)\n","\n","    # Carregar imagens de tratores (classe 1)\n","    for img_name in os.listdir(trator_dir):\n","        if img_name.endswith(('.jpg', '.jpeg', '.png')):\n","            img_path = os.path.join(trator_dir, img_name)\n","            img = load_img(img_path, target_size=(150, 150))\n","            img_array = img_to_array(img) / 255.0\n","            imagens.append(img_array)\n","            rotulos.append(1)\n","\n","    return np.array(imagens), np.array(rotulos)\n"],"metadata":{"id":"kgv0vhNliIox"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Carregamentos dos dados e divisão do treino"],"metadata":{"id":"G4AF2tWu1ONL"}},{"cell_type":"code","source":["# carregando imagens e rótulos\n","X, y = carregar_dataset(drone_dir, trator_dir)\n","\n","# Divisão treino e teste\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Converter rótulos para one-hot encoding\n","y_train = to_categorical(y_train, 2)\n","y_test = to_categorical(y_test, 2)\n"],"metadata":{"id":"rHRE37dqnkUR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Aplicando data augmentation nos dados de treino"],"metadata":{"id":"G6wNLrYH1ZkD"}},{"cell_type":"code","source":["# Gerador de alterações treino\n","train_datagen = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","# Gerador teste sem aplicação de alterações nas imagens\n","test_datagen = ImageDataGenerator()\n","\n","# Preparar geradores de lotes para treino e teste\n","train_generator = train_datagen.flow(\n","    X_train, y_train,\n","    batch_size = 16\n",")\n","\n","test_generator = test_datagen.flow(\n","    X_test, y_test,\n","    batch_size= 16\n",")\n"],"metadata":{"id":"Sar-sAuOovd6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Estruturando a CNN"],"metadata":{"id":"T4IWvKBx2ERV"}},{"cell_type":"code","source":["# Estrutura da CNN\n","model = Sequential([\n","    # 3 camdadas convolucionais\n","    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape= (150, 150, 3)),\n","    BatchNormalization(),\n","    MaxPooling2D(pool_size=(2, 2)),\n","    Dropout(0.25),\n","\n","    Conv2D(64, (3, 3), activation='relu', padding='same'),\n","    BatchNormalization(),\n","    MaxPooling2D(pool_size=(2, 2)),\n","    Dropout(0.25),\n","\n","    Conv2D(128, (3, 3), activation='relu', padding='same'),\n","    BatchNormalization(),\n","    MaxPooling2D(pool_size=(2, 2)),\n","    Dropout(0.25),\n","\n","    Flatten(),\n","\n","    # 1 camada densa\n","    Dense(512, activation='relu'),\n","    BatchNormalization(),\n","    Dropout(0.5),\n","\n","    # Camada de saída\n","    Dense(2, activation='softmax')\n","])\n","\n","model.compile(\n","    optimizer=Adam(learning_rate=0.0001),\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    patience=10,\n","    restore_best_weights=True\n",")\n","\n","model_checkpoint = ModelCheckpoint(\n","    'melhor_modelo.h5',\n","    monitor='val_accuracy',\n","    save_best_only=True\n",")"],"metadata":{"id":"A9e6B2Fuqsv7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Treino do modelo"],"metadata":{"id":"rMO0HJu-2Sg5"}},{"cell_type":"code","source":["history = model.fit(\n","    train_generator,\n","    steps_per_epoch=len(X_train) // 16,\n","    epochs=25,\n","    validation_data=test_generator,\n","    validation_steps=len(X_test) // 16,\n","    callbacks=[early_stopping, model_checkpoint]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXrDTMi5sFUg","executionInfo":{"status":"ok","timestamp":1745895616168,"user_tz":180,"elapsed":20554,"user":{"displayName":"Pedro E. Sousa","userId":"02003335413265452009"}},"outputId":"4281c38e-e233-4431-e90c-f51c617a8a71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4s/step - accuracy: 0.4822 - loss: 1.5254 "]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3s/step - accuracy: 0.5216 - loss: 1.3659 - val_accuracy: 0.4667 - val_loss: 0.6776\n","Epoch 2/25\n","\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8750 - loss: 0.3244"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step - accuracy: 0.8750 - loss: 0.3244 - val_accuracy: 0.5333 - val_loss: 0.6762\n","Epoch 3/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8984 - loss: 0.4629 - val_accuracy: 0.5333 - val_loss: 0.6767\n","Epoch 4/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8889 - loss: 0.7582 - val_accuracy: 0.5333 - val_loss: 0.6760\n","Epoch 5/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9167 - loss: 0.1497"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 500ms/step - accuracy: 0.9219 - loss: 0.1444 - val_accuracy: 0.6000 - val_loss: 0.6702\n","Epoch 6/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6667 - loss: 1.2277 - val_accuracy: 0.6000 - val_loss: 0.6628\n","Epoch 7/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7674 - loss: 0.5604"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 462ms/step - accuracy: 0.7839 - loss: 0.5345 - val_accuracy: 0.7333 - val_loss: 0.6459\n","Epoch 8/25\n","\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7778 - loss: 0.3968"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407ms/step - accuracy: 0.7778 - loss: 0.3968 - val_accuracy: 0.8000 - val_loss: 0.6454\n","Epoch 9/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7269 - loss: 0.4438 - val_accuracy: 0.5333 - val_loss: 0.6517\n","Epoch 10/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8125 - loss: 0.3752 - val_accuracy: 0.5333 - val_loss: 0.6528\n","Epoch 11/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8765 - loss: 0.3611 - val_accuracy: 0.6667 - val_loss: 0.6501\n","Epoch 12/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9375 - loss: 0.1234 - val_accuracy: 0.6667 - val_loss: 0.6499\n","Epoch 13/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8712 - loss: 0.2778 - val_accuracy: 0.6667 - val_loss: 0.6348\n","Epoch 14/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.0811 - val_accuracy: 0.7333 - val_loss: 0.6314\n","Epoch 15/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8828 - loss: 0.4277 - val_accuracy: 0.5333 - val_loss: 0.6341\n","Epoch 16/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8889 - loss: 0.4765 - val_accuracy: 0.5333 - val_loss: 0.6407\n","Epoch 17/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7188 - loss: 0.8725 - val_accuracy: 0.5333 - val_loss: 0.6486\n","Epoch 18/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7778 - loss: 0.6728 - val_accuracy: 0.5333 - val_loss: 0.6500\n","Epoch 19/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9400 - loss: 0.1283 - val_accuracy: 0.5333 - val_loss: 0.6666\n","Epoch 20/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9375 - loss: 0.3434 - val_accuracy: 0.5333 - val_loss: 0.6663\n","Epoch 21/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8516 - loss: 0.3035 - val_accuracy: 0.5333 - val_loss: 0.6656\n","Epoch 22/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0323 - val_accuracy: 0.5333 - val_loss: 0.6613\n","Epoch 23/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9656 - loss: 0.1083 - val_accuracy: 0.5333 - val_loss: 0.6774\n","Epoch 24/25\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9375 - loss: 0.0590 - val_accuracy: 0.5333 - val_loss: 0.6772\n"]}]},{"cell_type":"markdown","source":["## Acurácia do modelo"],"metadata":{"id":"izTXqqeR3GrX"}},{"cell_type":"code","source":["test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"Acurácia no conjunto de teste: {test_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q2WpmLIytBVf","executionInfo":{"status":"ok","timestamp":1745895621905,"user_tz":180,"elapsed":666,"user":{"displayName":"Pedro E. Sousa","userId":"02003335413265452009"}},"outputId":"e52de280-2e44-43e9-d8da-a519b7cad24f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 543ms/step - accuracy: 0.7333 - loss: 0.6314\n","Acurácia no conjunto de teste: 73.33%\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.models import load_model\n","\n","# Carregar o modelo treinado\n","model = load_model('melhor_modelo.h5')\n","\n","# Pasta com as imagens para previsão\n","pasta_previsoes = '/content/drive/MyDrive/Entregavel_2/teste'\n","\n","# Lista para armazenar as previsões\n","previsoes = []\n","\n","# Iterar pelas imagens na pasta\n","for nome_arquivo in os.listdir(pasta_previsoes):\n","    if nome_arquivo.endswith(('.jpg', '.jpeg', '.png')):\n","        caminho_imagem = os.path.join(pasta_previsoes, nome_arquivo)\n","        try:\n","            img = load_img(caminho_imagem, target_size=(150, 150))\n","            img_array = img_to_array(img) / 255.0\n","            img_array = np.expand_dims(img_array, axis=0)\n","\n","            # Fazer a previsão\n","            predicao = model.predict(img_array)\n","            classe_predita = np.argmax(predicao)\n","            previsoes.append((nome_arquivo, classe_predita))\n","\n","        except Exception as e:\n","            print(f\"Erro ao processar a imagem {nome_arquivo}: {e}\")\n","\n","# Imprimir as previsões\n","for nome_arquivo, classe_predita in previsoes:\n","    print(f\"Imagem: {nome_arquivo}, Classe Predita: {classe_predita}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ssh53APRt2LW","executionInfo":{"status":"ok","timestamp":1745895564499,"user_tz":180,"elapsed":1366,"user":{"displayName":"Pedro E. Sousa","userId":"02003335413265452009"}},"outputId":"a59cb902-b76f-406e-e71b-fdf5fabda53b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","Imagem: teste 1.jpg, Classe Predita: 1\n","Imagem: teste 2.jpg, Classe Predita: 0\n","Imagem: teste 3.jpg, Classe Predita: 1\n","Imagem: teste 4.jpeg, Classe Predita: 0\n","Imagem: teste 5.jpg, Classe Predita: 0\n","Imagem: teste 6.jpg, Classe Predita: 1\n","Imagem: teste 7.jpg, Classe Predita: 0\n","Imagem: teste 8.jpg, Classe Predita: 1\n"]}]},{"cell_type":"markdown","source":["## Relatório\n","\n","O código implementa uma Rede Neural Convolucional (CNN) para classificar\n","80 imagens de drones e tratores, sendo 40 imagens por objeto. A análise dos resultados obtidos será dividida em etapas para maior clareza:\n","\n","1. Arquitetura da CNN:\n","\n","A CNN possui 3 camadas convolucionais, intercaladas com camadas de Batch Normalization, Max Pooling e Dropout para regularização e evitar overfitting. A arquitetura termina com uma camada densa e uma camada de saída com função de ativação softmax para classificação binária.\n","\n","2. Data Augmentation:\n","\n","Foi utilizada técnica de Data Augmentation para aumentar a variabilidade dos dados de treino, incluindo rotações, translações e flips horizontais, tornando o modelo mais robusto.\n","\n","3. Treinamento e Avaliação:\n","\n","O modelo foi treinado com o otimizador Adam e função de perda 'categorical_crossentropy'. Métricas de precisão foram utilizadas para avaliar o desempenho. Early Stopping e Model Checkpoint foram implementados para evitar overfitting e salvar o melhor modelo.\n","\n","4. Resultados:\n","\n","A acurácia no conjunto de teste foi de aproximadamente 73,33%. É importante notar que o valor específico da acurácia pode variar a cada execução do código devido à aleatoriedade no processo de treinamento e Data Augmentation.\n","\n","5. Análise:\n","\n","A CNN apresentou resultados promissores na classificação de drones e tratores. A utilização de técnicas como Data Augmentation, Batch Normalization e Dropout contribuíram para a generalização do modelo e evitar overfitting. O uso de Early Stopping e Model Checkpoint permitiu encontrar um bom ponto de parada no treinamento e salvar o modelo com melhor desempenho.\n","\n","7. Conclusão:\n","\n","A CNN desenvolvida demonstra potencial para classificar imagens de drones e tratores. A análise dos resultados indica um bom desempenho, com acurácia satisfatória no conjunto de teste. No entanto, há espaço para melhorias através de ajustes na arquitetura, hiperparâmetros e técnicas de treinamento.\n"],"metadata":{"id":"ypDeFAOtCvKU"}},{"cell_type":"markdown","source":["## Análise Comparativa: Modelo de Classificação (CNN) vs. Detectores de Objetos (YOLOv12 e YOLOv5)\n","\n","A distinção crucial reside na tarefa que cada tipo de modelo foi projetado para realizar:\n","\n","Modelo de Classificação (CNN): O objetivo principal de um modelo de classificação de imagens é atribuir um rótulo de classe a uma imagem inteira. Ele aprende a identificar as características visuais mais salientes que discriminam entre as categorias predefinidas. A métrica de avaliação primária é a acurácia, que mede a proporção de imagens corretamente classificadas. No nosso caso, a acurácia de 73.33% indica o desempenho do modelo em dizer \"o que está na imagem\". A perda (loss) complementa essa avaliação, quantificando o erro na atribuição das probabilidades das classes.\n","\n","Modelos de Detecção de Objetos (YOLOv12 e YOLOv5): O objetivo desses modelos vai além da simples classificação. Eles visam identificar a presença de múltiplos objetos dentro de uma imagem, localizar cada objeto com uma caixa delimitadora e classificar cada objeto detectado. A saída não é apenas um rótulo para a imagem como um todo, mas sim múltiplos rótulos (um para cada objeto) juntamente com informações espaciais (as coordenadas das caixas delimitadoras). As métricas de avaliação primárias são a Mean Average Precision (mAP) em diferentes limiares de IoU (mAP50 e mAP50-95), que levam em conta tanto a precisão da classificação dos objetos detectados quanto a precisão da sua localização.\n","\n","Comparação Direta e Implicações Técnicas:\n","\n","Natureza da Saída: A saída de um classificador é uma única distribuição de probabilidade sobre as classes para a imagem de entrada. A saída de um detector de objetos é um conjunto de detecções, onde cada detecção inclui uma caixa delimitadora (definida por suas coordenadas), uma pontuação de confiança e uma probabilidade de classe. Essa diferença fundamental na natureza da saída reflete a complexidade inerentemente maior da tarefa de detecção.\n","\n","Complexidade da Arquitetura e da Tarefa: As arquiteturas dos detectores de objetos, como YOLO, são geralmente mais complexas que as dos classificadores de imagem tradicionais. Elas precisam incorporar mecanismos para lidar com a variabilidade no número, tamanho, forma e localização dos objetos dentro de uma imagem. As camadas convolucionais servem como base para ambos, mas os detectores YOLO adicionam camadas especializadas para a previsão das caixas delimitadoras e a associação das predições às âncoras.\n","\n","Métricas de Avaliação: A acurácia de um classificador não é diretamente comparável ao mAP de um detector de objetos. A acurácia avalia a correção da previsão da classe dominante na imagem, enquanto o mAP avalia a precisão e a revocação das detecções de múltiplos objetos, levando em conta a sobreposição espacial com as anotações verdadeiras. Um mAP alto indica que o detector é bom em encontrar todos os objetos relevantes (revocação) e que as detecções que ele faz são geralmente corretas (precisão), tanto em termos de classe quanto de localização.\n","Desempenho Relativo: Os resultados mostram que os modelos YOLO (v12 e v5) alcançam um mAP geral significativamente alto (acima de 90% em mAP50 para o melhor desempenho de cada modelo), indicando um bom desempenho na tarefa de detecção de tratores e drones. A acurácia do classificador (73.33%) reflete seu sucesso na tarefa de classificação, mas não informa sobre sua capacidade de localizar objetos individuais dentro da imagem ou de identificar múltiplos objetos.\n","\n","Implicações Técnicas para os Resultados:\n","\n","A alta acurácia do classificador sugere que as características distintivas das classes que ele foi treinado para reconhecer são bem aprendidas pela sua arquitetura CNN.\n","\n","O alto mAP alcançado pelos modelos YOLO demonstra a eficácia das suas arquiteturas CNN especializadas em realizar a tarefa mais complexa de detecção de objetos, incluindo a localização precisa e a classificação simultânea. As diferenças no mAP entre YOLOv12 e YOLOv5 provavelmente se devem a variações nas suas arquiteturas CNN subjacentes, nas suas estratégias de previsão e nas suas funções de perda otimizadas para a detecção.\n","\n","Conclusão da Comparação CNN (Classificação) vs. YOLO (Detecção):\n","\n","Em essência, comparar a acurácia de um classificador com o mAP de um detector de objetos é como comparar maçãs com laranjas. Eles resolvem problemas fundamentalmente diferentes na visão computacional. Os resultados indicam que ambos os tipos de modelos foram capazes de aprender com sucesso as representações necessárias para suas respectivas tarefas. Os modelos YOLO, com suas arquiteturas CNN mais complexas e métricas de avaliação especializadas, demonstram a capacidade de não apenas classificar objetos, mas também de localizá-los com precisão dentro de uma imagem, uma capacidade que um classificador tradicional não possui. A escolha entre um classificador e um detector de objetos depende inteiramente do objetivo da aplicação: identificar o conteúdo geral de uma imagem ou identificar e localizar objetos específicos dentro dela."],"metadata":{"id":"vrmbdD_BF0u2"}}]}